{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Finetuning and Prompting\n",
    "\n",
    "In this project, you will first learn how to use Huggingface's Transformers library to load large language models. Next, we will generate text from these models. Finally, we will work with a small text-to-SQL dataset, where the input is a natural language query and the output is a SQL query that can be executed against a database. You will explore (1) finetuning a pretrained language model, and (2) prompting the pretrained language model with examples. \n",
    "\n",
    "This project will be more open ended than the previous projects. We expect you to learn how to use the huggingface and torch documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we install and import the required dependencies. These include:\n",
    "* `torch` for modeling and training\n",
    "* `transformers` for pre-trained models\n",
    "* `datasets` from huggingface to load existing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install sentence-transformers\n",
    "\n",
    "# Standard library imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, let's verify that we're connected to a GPU runtime and that `torch` can detect the GPU.\n",
    "We'll define a variable `device` here to use throughout the code so that we can easily change to run on CPU for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use GPT-2 Medium for this project. This includes both the GPT-2 tokenizer and the GPT-2 model weights itself. If you want to learn more about this model, you can read the GPT-2 paper https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.\n",
    "\n",
    "Let's first load the tokenizer for the GPT-2 medium model. You can find how to do this by reading the documentation for AutoTokenzier in transformers, and finding the GPT-2 model of ~345 million params in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token # convenient for padding later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize and detokenize some text from this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 995]\n",
      "Hello world\n",
      "[39, 5708, 11, 269, 10205, 5908, 1556, 40138, 47249, 235]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode('Hello world'))\n",
    "print(tokenizer.decode(tokenizer.encode('Hello world')))\n",
    "print(tokenizer.encode(\"Hola, c√≥mo est√°süòç\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the GPT-2 Medium model. Make sure you also put the model onto the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "gpt2_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate From the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate some text from the model to test its LM capabilities. Let's first generate one output of length 50 tokens using greedy decoding (temperature = 0), which should get us some text with high likelihood under the model. When generating text, you can condition on phrases such as \"The coolest thing in NLP right now is\". Find the relevant function and arguments to use for generating text using the Huggingface documentation.\n",
    "\n",
    "Hint: you may find https://huggingface.co/docs/transformers/main_classes/text_generation to be useful for learning about generating from LMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  464, 38889,  1517,   826,   783,   287,   399, 19930,   318,   262,\n",
      "         2694,   284,   779,   262,   976,  1366,   284,  4331,   262,  2003,\n",
      "           13,   198,   198,   464,  2003,   318,  1464,  5609,    11,   290,\n",
      "          262,  1366,   318,  1464,  5609,    13,   198,   198,   464,  2003,\n",
      "          318,  1464,  5609,    11,   290,   262,  1366,   318,  1464,  5609],\n",
      "       device='cuda:0')\n",
      "The coolest thing right now in NLP is the ability to use the same data to predict the future.\n",
      "\n",
      "The future is always changing, and the data is always changing.\n",
      "\n",
      "The future is always changing, and the data is always changing\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"The coolest thing right now in NLP is\", return_tensors=\"pt\").input_ids.cuda()\n",
    "# Your code here\n",
    "\n",
    "attention_mask = (inputs != tokenizer.pad_token_id).float()\n",
    "sample_output = gpt2_model.generate(inputs, max_length=50, pad_token_id=tokenizer.pad_token_id, attention_mask=attention_mask, temperature=0, num_return_sequences=1)[0]\n",
    "print(sample_output)\n",
    "print(\"{}\".format(tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate 10 pieces of random text of length 50 tokens from the model using random sampling with temperature set to 0.7. This will allow the text to be somewhat higher in diversity (random sampling) while maintaining reasonable quality (temperature < 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: The coolest thing right now in NLP is the fact that you can easily create machine learning models that are able to predict different things, without needing to write a lot of code,\" says Karp. \"The data can be stored in a graph,\n",
      "1: The coolest thing right now in NLP is in the field of behavioral analysis,\" said Dr. Michael Scholz, a lecturer in linguistics at the University of California, Los Angeles, and the author of the book The Language of Language. \"\n",
      "2: The coolest thing right now in NLP is that it's the first thing people are saying to me. It's the first thing that people who don't know anything about NLP say to me. I'm pretty happy about that. It's really\n",
      "3: The coolest thing right now in NLP is that you can also use it to control your own personal data. It's used to track your diet, sleep patterns, stress levels, mood, and so on.\n",
      "\n",
      "It's also used to record\n",
      "4: The coolest thing right now in NLP is that you can ask the right questions, and get results. You don't need to know the answers, you just need to ask the right questions.\n",
      "\n",
      "Question #2: \"What can I do\n",
      "5: The coolest thing right now in NLP is that it's not just a bunch of little things that we're doing. We're also working on the whole idea of machine learning and artificial intelligence and natural language processing. It's not just some specific thing\n",
      "6: The coolest thing right now in NLP is that it's very much a problem solving language with lots of interesting features. You can start using it to solve problems in your own code and make it more expressive, and you'll get a lot of interesting\n",
      "7: The coolest thing right now in NLP is that we're gaining a lot of the insights and techniques that are important for building better products, better customer experience, better customer experience, better products, better products. We're gaining more and more data and\n",
      "8: The coolest thing right now in NLP is the fact that you can take a few words and turn them into a sentence, and you can combine the two to make something amazing.\n",
      "\n",
      "I'm not saying that all this is science fiction, but\n",
      "9: The coolest thing right now in NLP is the ability to automatically determine the best language in different contexts, and to use these languages as inputs to other programs.\n",
      "\n",
      "One of the big questions that I'm looking for is \"how should I structure\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"The coolest thing right now in NLP is\", return_tensors=\"pt\").input_ids.cuda()\n",
    "# Your code here\n",
    "\n",
    "attention_mask = (inputs != tokenizer.pad_token_id).float()\n",
    "sample_outputs = gpt2_model.generate(inputs, max_length=50, pad_token_id=tokenizer.pad_token_id, attention_mask=attention_mask, temperature=0.7, num_return_sequences=10, do_sample=True)\n",
    "\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-to-SQL Task Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's download the data of text-to-SQL pairs and the database against which we'll execute queries to retrieve answers.\n",
    "\n",
    "The code below initializes the database and does some initial preprocessing data preprocessing + splitting for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!curl https://github.com/jkkummerfeld/text2sql-data/raw/master/data/geography.json\n",
    "#!curl https://github.com/jkkummerfeld/text2sql-data/raw/master/data/geography-db.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import re\n",
    "import sqlite3\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "DATABASE_NAME = 'geo.db'\n",
    "SQL_FILENAME = 'geography-db.sql'\n",
    "DATASET_FILENAME = 'geography.json'\n",
    "\n",
    "with open(SQL_FILENAME, 'r') as file:\n",
    "    sql_script = file.read()\n",
    "    sql_script = re.sub(r\"\\s*ENGINE=[^ ]+\",\"\", sql_script)\n",
    "    sql_script = re.sub(r\"\\s*DEFAULT CHARSET=[^ ;]+\",\"\", sql_script)\n",
    "    sql_script = re.sub(r\"\\s*LOCK TABLES `[^`]+` WRITE;\",\"\", sql_script)  # remove LOCK TABLES\n",
    "    sql_script = re.sub(r\"\\s*UNLOCK TABLES;\",\"\", sql_script)  # remove UNLOCK TABLES\n",
    "    sql_script = sql_script.replace('`', '')  # remove backticks\n",
    "\n",
    "# Connect to the SQLite database (this will create the file if it doesn't exist)\n",
    "connection = sqlite3.connect(DATABASE_NAME)\n",
    "print(sql_script)\n",
    "\n",
    "connection.executescript(sql_script)\n",
    "connection.commit()\n",
    "connection.close()\n",
    "\n",
    "connection = sqlite3.connect(DATABASE_NAME)\n",
    "cursor = connection.cursor()\n",
    "with open(DATASET_FILENAME, 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "splits = {'train': [], 'dev': [], 'test': []}\n",
    "for query_type in dataset:\n",
    "    for example in query_type['sentences']:\n",
    "        split = example['question-split']\n",
    "        example['question'] = example['text']\n",
    "        for key, value in example['variables'].items():\n",
    "            example['question'] = example['question'].replace(key, value)\n",
    "        example['sql'] = deepcopy(query_type['sql'])\n",
    "        example['sql'] = example['sql'][0]\n",
    "        for key, value in example['variables'].items():\n",
    "            example['sql'] = example['sql'].replace(key, value)\n",
    "        try:\n",
    "            cursor.execute(example['sql'])\n",
    "        except:\n",
    "            continue\n",
    "        example['db_answer'] = cursor.fetchall()\n",
    "        del example['text']\n",
    "        del example['variables']\n",
    "        splits[split].append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide a function you can use to query the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_db(sql):\n",
    "    connection = sqlite3.connect(DATABASE_NAME)\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(sql)\n",
    "    result = cursor.fetchall()\n",
    "    connection.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98861555a7554fca9449a78758ae2611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3c46dc6af3413b9c3f742ebed74fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/649k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a91e07b99d420793751a3241d4d690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/75.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1c45a6f11b4c76baf2770911dae84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/308k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2c6009667643fba48c80f3c96c511b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3ac9b06d6046aa8679e216e7c1c03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4505f22f17894fa78ef0dcc863b5de55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data=load_dataset(\"glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data['train'][0]))\n",
    "print(type(data['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "data_list=splits['train']\n",
    "\n",
    "dict_data = {\"question\": [item[\"question\"] for item in data_list], \"sql\": [item[\"sql\"] for item in data_list]}\n",
    "dataset = datasets.Dataset.from_dict(dict_data)\n",
    "\n",
    "print(type(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question-split': 'train', 'question': 'what is the biggest city in nebraska', 'sql': 'SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"nebraska\" ) AND CITYalias0.STATE_NAME = \"nebraska\" ;', 'db_answer': [('omaha',)]}\n",
      "<class 'datasets.dataset_dict.DatasetDict'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print((splits['train'][0]))\n",
    "x=DatasetDict(splits)\n",
    "print(type(x))\n",
    "print(type(x['train']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is pretty small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 547\n",
      "Dev set size: 48\n",
      "Test set size: 277\n"
     ]
    }
   ],
   "source": [
    "print('Train set size:', len(splits['train']))\n",
    "print('Dev set size:', len(splits['dev']))\n",
    "print('Test set size:', len(splits['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect an example from the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question-split': 'train',\n",
       " 'question': 'what is the biggest city in nebraska',\n",
       " 'sql': 'SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"nebraska\" ) AND CITYalias0.STATE_NAME = \"nebraska\" ;',\n",
       " 'db_answer': [('omaha',)]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `db_answer` is the result of executing the given SQL output against the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('omaha',)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db(splits['train'][0]['sql'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how well our language model does on this text-to-SQL task out of the box. You can just use greedy decoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a SQL query based on the following question.\\n\\nQuestion: {input}\\n\\nSQL:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a SQL query based on the following question.\n",
      "\n",
      "Question: {input}\n",
      "\n",
      "SQL: SELECT * FROM {input}\n",
      "\n",
      "Output:\n",
      "\n",
      "SELECT * FROM {input}\n",
      "\n",
      "Question: {input}\n",
      "\n",
      "SQL:\n"
     ]
    }
   ],
   "source": [
    "# Your code here. Generate from the model using greedy decoding with the above prompt\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "attention_mask = (inputs != tokenizer.pad_token_id).float()\n",
    "output = gpt2_model.generate(inputs, max_length=50, pad_token_id=tokenizer.pad_token_id, attention_mask=attention_mask, temperature=0, num_return_sequences=1)[0]\n",
    "\n",
    "\n",
    "predicted_sql = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "print(predicted_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get something that looks kind of like a SQL query, but it probably won't match the correct output, and in fact it most likely won't even execute without crashing when you try to query the database (you'll see a syntax error below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to execute!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    query_db(predicted_sql)\n",
    "    print('success!')\n",
    "except:\n",
    "    print('failed to execute!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm quantitatively that the model doesn't work well out-of-the-box by running on the dev dataset (`splits['dev']`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_greedy(model, data, max_new_tokens=128):\n",
    "    \"\"\"\n",
    "    Return the model's greedy text-to-sql predictions on the given data split.\n",
    "    The maximum number of new tokens generated (NOT including tokens in the prompt) should be equal to max_new_tokens.\n",
    "    For speed, you should batch the generation. The tokenizer can handle multiple inputs simultaneously,\n",
    "    but you'll need to tell it to pad using padding=True, and you may also need to set tokenizer.padding_side='left'.\n",
    "    Hint: as a postprocessing step after you're done, you may need to cut off the output at the first appearance of '\\n' if the output is continuing past the end of the SQL.\n",
    "    \"\"\"\n",
    "    questions = [d['question'] for d in data]\n",
    "    predicted_sqls = []\n",
    "    # Your code here\n",
    "    batch_size=4\n",
    "    model=model.to(device)\n",
    "    model.eval()\n",
    "    tokenizer.padding_side='left'\n",
    "    PROMPT = \"Write a SQL query based on the following question.\\n Question: {question}\\n\\nSQL:\"\n",
    "    questions=[PROMPT.format(question=i) for i in questions]\n",
    "\n",
    "    for i in range(0, len(questions), batch_size):\n",
    "        batch_questions = questions[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_questions, return_tensors=\"pt\", padding=True).input_ids.cuda()\n",
    "\n",
    "        attention_mask = (torch.ones_like(inputs)).float()\n",
    "        output = model.generate(inputs, max_length=max_new_tokens,  pad_token_id=tokenizer.pad_token_id, attention_mask=attention_mask, num_return_sequences=1, temperature=0,do_sample=False)\n",
    "        generated_sqls = tokenizer.batch_decode(output, skip_special_tokens=True, max_length=max_new_tokens)\n",
    "        # print(generated_sqls)\n",
    "        # print( )\n",
    "        \n",
    "        \n",
    "        for i in range(len(generated_sqls)):\n",
    "            if '\\n\\n' in generated_sqls[i]:\n",
    "                components = generated_sqls[i].split('\\n\\n')\n",
    "                predicted_sqls.append(components[1].replace(\"SQL: \", \"\"))\n",
    "        # print(predicted_sqls)\n",
    "        # print(\"####################################################\")\n",
    "\n",
    "    return predicted_sqls # list of strings containing SQL predictions for each question in the data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_execution_accuracy(predictions, data):\n",
    "    assert len(predictions) == len(data)\n",
    "    correct = 0\n",
    "    for p, d in zip(predictions, data):\n",
    "        try:\n",
    "            if query_db(p) == d['db_answer']:\n",
    "                correct += 1\n",
    "        except: # failed to execute\n",
    "            pass\n",
    "    return correct / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example prediction: [\"SELECT city FROM city WHERE city.name = 'Arizona' AND city.zipcode = 'AZ'\", \"SELECT * FROM city WHERE name LIKE 'Dallas' AND city_id = 1;\", \"SELECT city FROM city WHERE city.name = 'Missouri' AND city.population > 1000000\", \"SELECT * FROM rivers WHERE city = 'New York'\", \"SELECT * FROM texas WHERE name LIKE 'Texas' ORDER BY name DESC LIMIT 1;\", \"SELECT * FROM area WHERE area.name = 'california'\", \"SELECT * FROM mytable WHERE name = 'new mexico'\", \"SELECT * FROM people WHERE name LIKE 'WASHINGTON'\", \"SELECT * FROM population WHERE state = 'ALABAMA'\", \"SELECT * FROM state WHERE state.capital = 'albany'\", 'SELECT * FROM lakes WHERE id = 1;', \"SELECT * FROM states WHERE state = 'Alabama' AND state = 'Georgia' AND state = 'Louisiana' AND state = 'Mississippi' AND state = 'Tennessee' AND state = 'Texas' AND state = 'Utah' AND state = 'Virginia' AND state = 'Washington' AND state = 'West Virginia' AND state = 'Wisconsin' AND state = 'Wyoming' AND state = 'Wyoming State' WHERE state = 'Alabama'\", \"SELECT * FROM rivers WHERE state = 'colorado'\", \"SELECT state_id, state_name, state_location FROM state_id WHERE state_name = 'colorado' AND state_location = 'florence'\", \"SELECT state FROM state WHERE state.state_id = 'Mississippi' AND state.state_name = 'Mississippi' AND state.state_zip = 'Mississippi' AND state.state_zip2 = 'Mississippi' AND state.state_zip3 = 'Mississippi' AND state.state_zip4 = 'Mississippi' AND state.state_zip5 = 'Mississippi' AND state.state_zip6 = 'Miss\", 'SELECT state.population FROM population WHERE state.population > 100000', 'SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT', 'SELECT * FROM state_by_state WHERE state_lowest_point = sea_level;', 'SELECT * FROM rivers WHERE length > 10', 'SQL:', 'SELECT * FROM rivers WHERE id = 1', \"SELECT * FROM maine WHERE state ='maine'\", \"SELECT state FROM state_id WHERE state_id = 'iowa'\", 'SQL:', 'SELECT state_id, state_name FROM states WHERE state_id = 1;', \"SELECT state FROM state WHERE state.state_id = 'DALLAS' AND state.state_name = 'DALLAS' AND state.state_zip = 'US' AND state.state_zip2 = 'US' AND state.state_zip3 = 'US' AND state.state_zip4 = 'US' AND state.state_zip5 = 'US' AND state.state_zip6 = 'US' AND state.state_zip7\", \"SELECT * FROM san diego WHERE name LIKE 'San Diego'\", 'SELECT * FROM state WHERE population > 100', \"SELECT * FROM people WHERE name LIKE 'chicago' ORDER BY name DESC LIMIT 1;\", 'SELECT * FROM dallas WHERE id = 1;', \"SELECT city FROM city WHERE city.name = 'arkansas' AND city.zipcode = '7'\", \"SELECT * FROM rivers WHERE river_name = 'New York'\", 'SELECT * FROM population WHERE population_density > 0 ORDER BY population_density DESC LIMIT 1;', \"SELECT * FROM points WHERE state ='red' AND colorado_state ='red'\", 'SELECT * FROM mytable WHERE elevation > 10', 'SELECT * FROM florida WHERE height > 10;', 'SELECT * FROM rivers WHERE id = 1', \"SELECT * FROM city WHERE state = 'ALABAMA'\", \"SELECT city FROM city WHERE state = 'MA' AND state = 'DE' AND state = 'DE' AND state = 'DE' AND state = 'DE' AND state = 'DE' AND state = 'DE' AND state = 'DE' AND state = 'DE' AND state = 'DE' AND state = 'DE' AND state = 'DE' AND state = 'DE' AND state = 'DE' AND state = 'DE' AND state = 'DE' AND\", \"SELECT state FROM state WHERE state.name = 'Alabama' AND state.population > 1000000;\", \"SELECT * FROM states WHERE state = 'Mississippi' AND state = 'Alabama' AND state = 'Georgia' AND state = 'South Carolina' AND state = 'Tennessee' AND state = 'Texas' AND state = 'Utah' AND state = 'Virginia' AND state = 'Washington' AND state = 'West Virginia' AND state = 'Wisconsin' AND state = 'Arkansas' AND state = 'Missouri' AND state = '\", 'SELECT * FROM population WHERE population > 1000000', \"SELECT * FROM rivers WHERE state = 'WA' AND state = 'CA' AND state = 'OR' AND state = 'WA' AND state = 'CA' AND state = 'OR' AND state = 'WA' AND state = 'CA' AND state = 'OR' AND state = 'WA' AND state = 'CA' AND state = 'OR' AND state = 'WA' AND state = 'CA' AND state = 'OR\", 'SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT', \"SELECT state FROM state WHERE state.border_state = 'none'\", 'SELECT * FROM state WHERE area = 1;', 'SELECT * FROM atlanta WHERE id = 1;', 'SQL:']\n",
      "initial execution acc 0.0\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_greedy(gpt2_model, splits['dev'])\n",
    "print('example prediction:', predictions)\n",
    "print('initial execution acc', check_execution_accuracy(predictions, splits['dev']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will probably observe an accuracy around 0-2%. (It may be hard to verify if your `predict_greedy` function is correct at this stage, because the expected accuracy is so low, but you will reuse it later with an improved model, at which point it will be more obvious if your implementation is correct.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's prepare our dataset for finetuning (i.e., training our pretrained language model on this text-to-SQL training set). For each element in the dataset, it should have a text prompt and then the SQL output, similar to above. Your job is to fill in the labels field below. This field sets the labels to use for training during the language modeling task.\n",
    "\n",
    "For the labels, we only want to train the model to output the text after the word \"SQL:\". This is because in the prompt, everything before the word \"SQL:\" will also be provided to the model as input. Hint: use -100 as the label for tokens you do not want to train on. Hint 2: When doing LM training, the labels are the same as the input tokens, except shifted to the left by one. You should check whether Huggingface is already doing the shifting, or whether you need to do the shifting yourself.\n",
    "\n",
    "One thing to be careful of with all LMs is to make sure there are not extra spaces. So, the text should be formatted as like \"SQL: {sql output}\" not \"SQL: {sql output} \". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text2SQLDataset(Dataset):\n",
    "    PROMPT = \"Write a SQL query based on the following question.\\n\\nQuestion: {question}\\n\\nSQL: {sql}\"\n",
    "\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        tokenizer.padding_side = 'right'\n",
    "\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "\n",
    "        training_texts = []\n",
    "        for example in self.data:\n",
    "            training_text = Text2SQLDataset.PROMPT.format(question=example['question'], sql=example['sql']) + \"<|endoftext|>\" # include the end token so model knows when to stop!\n",
    "            training_texts.append(training_text)\n",
    "        encodings_dict = self.tokenizer(training_texts, padding=True, truncation=True)\n",
    "        for i, (example, training_text) in enumerate(zip(data, training_texts)):\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids'][i]))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask'][i]))\n",
    "            # Your code here\n",
    "            input_tokens = self.tokenizer.tokenize(training_text)\n",
    "            sql_start = input_tokens.index('SQL') \n",
    "            label = encodings_dict['input_ids'][i]\n",
    "            label[:sql_start]=[-100]*sql_start\n",
    "            self.labels.append(torch.tensor(label))\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids':(self.input_ids[idx]),'attn_masks': (self.attn_masks[idx]),'labels': (self.labels[idx])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ËøôÈáåÁöÑlabelÂëΩÂêç‰∏çÂ§™ÊÅ∞ÂΩìÔºåÂõ†‰∏∫Âπ∂‰∏çÊòØÊ†áÁ≠æÔºåËÄåÊòØÂøΩÁï•‰∫ÜPROMPTÈÉ®ÂàÜÔºåÂè™Êúâgpt2ÁîüÊàêÈÉ®ÂàÜÁöÑËæìÂá∫„ÄÇ\n",
    "ÂêåÊó∂ÈúÄË¶ÅÊ≥®ÊÑè__getitem__Â∫îËøîÂõûÂ≠óÂÖ∏Ê†ºÂºèËÄåÈùûÂàóË°®Ê†ºÂºè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Text2SQLDataset(splits['train'], tokenizer)\n",
    "dev_dataset = Text2SQLDataset(splits['dev'], tokenizer)\n",
    "test_dataset = Text2SQLDataset(splits['test'], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the Huggingface Trainer to finetune GPT-2 Medium on this dataset. This abstracts away all of the details of training. Setup the training arguments to perform 3 epochs of training on this dataset, use a per-device batch size of 2 with gradient accumulation set to 8, use 30 warmup steps, a weight decay of 0.05. Set the eval batch size to be 8. Save a checkpoint after 100 steps. Set fp16 to True. Save the checkpoint in a specific output_dir so you can load it later. Hint: if it tries to launch Wandb, you may add the argument report_to=\"none\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102' max='102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102/102 55:49, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.017300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=102, training_loss=0.0049981851377250515, metrics={'train_runtime': 3363.7195, 'train_samples_per_second': 0.488, 'train_steps_per_second': 0.03, 'total_flos': 892894929469440.0, 'train_loss': 0.0049981851377250515, 'epoch': 2.98})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"D:\\\\app_data\\\\huggingface_data\\\\my_model\\\\gpt_2\",\n",
    "    overwrite_output_dir=True,  \n",
    "    num_train_epochs=3,  \n",
    "    per_device_train_batch_size=2,  \n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=30, \n",
    "    weight_decay=0.05, \n",
    "    per_device_eval_batch_size=8,\n",
    "    save_steps=100, \n",
    "    evaluation_strategy=\"steps\",  \n",
    "    eval_steps=100,  \n",
    "    save_total_limit=2,  \n",
    "    report_to=\"none\", \n",
    "    fp16=True,   \n",
    "    load_best_model_at_end=True,  \n",
    "    logging_steps=100,\n",
    "    save_safetensors=False\n",
    ")\n",
    "\n",
    "data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer=Trainer(\n",
    "    model=gpt2_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    eval_dataset=dev_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the final saved version of the model below. You may need to delete the previously loaded model if you run out of GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del gpt2_model\n",
    "# Your code here\n",
    "\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(\"D:\\\\app_data\\\\huggingface_data\\\\my_model\\\\gpt_2\\\\checkpoint-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our finetuned model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetuned execution acc: 0.5631768953068592\n"
     ]
    }
   ],
   "source": [
    "finetuned_predictions = predict_greedy(finetuned_model, splits['test'])\n",
    "print('finetuned execution acc:', check_execution_accuracy(finetuned_predictions, splits['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should achieve an accuracy of roughly 50% using the suggested training hyperparameters; we will check >40% in the autograder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_predictions(predictions, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('\\n'.join(predictions))\n",
    "\n",
    "# save_predictions(finetuned_predictions, 'finetuned_predictions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect some of your predictions compared to the correct outputs, and describe some common types of errors in your report. What fraction of errors are due to failing to execute (e.g., syntax error), and what fraction are due to executing but getting the wrong answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "length=len(finetuned_predictions)\n",
    "for i in range(length):\n",
    "    if finetuned_predictions[i]!=splits['test'][i]['sql']:\n",
    "        print(i,finetuned_predictions[i],\"\\n\",splits['test'][i]['sql'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine the exact match accuracy (i.e., requiring the predicted SQL string to exactly match the gold answer) rather than the execution accuracy (just checking whether the output of executing the SQL against the database is the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_exact_match_accuracy(predictions, data):\n",
    "    assert len(predictions) == len(data)\n",
    "    correct = 0\n",
    "    for p, d in zip(predictions, data):\n",
    "        if p == d['sql']:\n",
    "            correct += 1\n",
    "    return correct / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetuned exact match acc: 0.5090252707581228\n"
     ]
    }
   ],
   "source": [
    "print('finetuned exact match acc:', check_exact_match_accuracy(finetuned_predictions, splits['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact match accuracy will likely be close to the execution accuracy, but not exactly the same. What are some potential pros and cons of each metric? Discuss in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unload your finetuned model so that you don't run out of GPU memory later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "del finetuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final part of this project, you will explore few-shot prompting, i.e., simply prompting the pretrained language model out-of-the-box using a small number of examples rather than finetuning.\n",
    "\n",
    "First, let's try just selecting 4 examples completely at random from the training set. Rewrite your `predict_greedy` function to change the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "few_shot_prompt = \"Question: {question0}\\n\\nSQL: {sql0}\\n\\n\\n\\n\" + \\\n",
    "    \"Question: {question1}\\n\\nSQL: {sql1}\\n\\n\\n\\n\" + \\\n",
    "    \"Question: {question2}\\n\\nSQL: {sql2}\\n\\n\\n\\n\" + \\\n",
    "    \"Question: {question3}\\n\\nSQL: {sql3}\\n\\n\\n\\n\" + \\\n",
    "    \"Question: {question}\\n\\nSQL:\"\n",
    "\n",
    "\n",
    "def select_random_examples(question, few_shot_data, num_examples=4):\n",
    "    \"\"\"\n",
    "    Return a list containing 4 of the elements of few_shot_data, selected randomly\n",
    "    \"\"\"\n",
    "    return random.sample(few_shot_data, num_examples)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_greedy_fewshot(model, data, few_shot_data, max_new_tokens=1024, example_selection_method=select_random_examples):\n",
    "    \"\"\"\n",
    "    Return the model's greedy text-to-sql predictions on the given data split.\n",
    "    The maximum number of new tokens generated (NOT including tokens in the prompt) should be equal to max_new_tokens.\n",
    "    The four examples with their SQL outputs should go in {question1}, {sql1}, {question2}, {sql2}, etc. in the few_shot_prompt. \n",
    "    The final {question} is the question that we're currently evaluating on.\n",
    "    \"\"\"\n",
    "    questions = [d['question'] for d in data]\n",
    "    predicted_sqls = []\n",
    "    prompts = []\n",
    "    for question in questions:\n",
    "        few_shot_examples = example_selection_method(question, few_shot_data, num_examples=4)\n",
    "        prompts.append(few_shot_prompt.format(\n",
    "            question0=few_shot_examples[0]['question'],\n",
    "            sql0=few_shot_examples[0]['sql'],\n",
    "            question1=few_shot_examples[1]['question'],\n",
    "            sql1=few_shot_examples[1]['sql'],\n",
    "            question2=few_shot_examples[2]['question'],\n",
    "            sql2=few_shot_examples[2]['sql'],\n",
    "            question3=few_shot_examples[3]['question'],\n",
    "            sql3=few_shot_examples[3]['sql'],\n",
    "            question=question\n",
    "        ))\n",
    "    batch_size=16\n",
    "    tokenizer.padding_side='left'\n",
    "    tmp=[]\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch_prompts = prompts[i:i + batch_size]\n",
    "        out_all = tokenizer.batch_encode_plus(batch_prompts, return_tensors=\"pt\", padding=True)\n",
    "        inputs = out_all.input_ids.cuda()\n",
    "        attention_mask = out_all.attention_mask.cuda()\n",
    "        output = model.generate(inputs, max_length=max_new_tokens, pad_token_id=tokenizer.pad_token_id, attention_mask=attention_mask, num_return_sequences=1, temperature=0,do_sample=False)\n",
    "        generated_sqls = tokenizer.batch_decode(output, skip_special_tokens=True, max_length=max_new_tokens)\n",
    "        for generated_sql in generated_sqls:\n",
    "            generated_sql_string = generated_sql.split(\"\\n\")\n",
    "            sql_strings = [s for s in generated_sql_string if s.startswith('SQL: ')]\n",
    "            tmp.append(sql_strings[4])\n",
    "\n",
    "\n",
    "    predicted_sqls = [sql.replace(\"SQL: \", \"\") for sql in tmp]\n",
    "    return predicted_sqls # list of strings containing SQL predictions for each question in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the gpt2 model if you need to\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained('gpt2-medium').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del gpt2_model\n",
    "torch.cuda.empty_cache()\n",
    "model_summary = torch.cuda.memory_summary()\n",
    "print(model_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-shot prompting with random examples, execution acc: 0.039711191335740074\n"
     ]
    }
   ],
   "source": [
    "# This call can take a few minutes even if you batch; you can debug on a subset of the dev set as needed.\n",
    "predictions = predict_greedy_fewshot(gpt2_model, splits['test'], splits['train'])\n",
    "print('4-shot prompting with random examples, execution acc:', check_execution_accuracy(predictions, splits['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will probably observe between 0-5% accuracy. Random example selection doesn't work very well on this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select_similar_examples\n",
    "However, what if we select examples by picking the examples from the training set whose questions are most similar to our current question? To do this, load a pretrained sentence encoder, which takes a sentence as input and outputs a fixed-length vector encoding semantic information about that sentence. First compute the vectors associated with all the training set questions, and then select examples from the training set based on which question vectors have the largest dot products with the vector for your current question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Your code here; load the sentence encoder (see https://www.sbert.net/ for documentation). A good choice of model is \"all-MiniLM-L6-v2\"\n",
    "sentence_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def compute_question_encodings(data):\n",
    "    \"\"\"\n",
    "    For each example in the data, add a field called 'question_encoding' to the example, which is the vector encoding of the question.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    vectors = sentence_encoder.encode([d['question'] for d in data],show_progress_bar=False)\n",
    "    for example, vector in zip(data, vectors):\n",
    "        example['question_encoding'] = np.array(vector)\n",
    "\n",
    "\n",
    "\n",
    "def select_similar_examples(question, few_shot_data, num_examples=4):\n",
    "    \"\"\"\n",
    "    Return a list containing 4 of the elements of few_shot_data, selected with questions most semantically similar to the given question. \n",
    "    The most similar question should be the LAST element of the list, second most similar should be the second to last element, etc.\n",
    "    The reason is that in the few-shot prompt, **you want the best example to be the most recent one.**\n",
    "\n",
    "    To rank by semantic similarity, first compute the vector for the current question, then compute its dot product with\n",
    "    all training set vectors (hint: you may want to vectorize this computation using numpy). Then sort by dot product.\n",
    "\n",
    "    You should take advantage of the 'question_encoding' field that you added to each example in compute_question_encodings.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # hint: when you call .encode with your sentence encoder, use show_progress_bar=False to avoid tons of printouts\n",
    "    current_question_encoding = sentence_encoder.encode([question], show_progress_bar=False)[0]\n",
    "\n",
    "    # Compute similarity scores with all examples\n",
    "    example_question_encodings = []\n",
    "    for example in few_shot_data:\n",
    "        example_question_encodings.append(example['question_encoding'])\n",
    "    example_question_encodings = np.array(example_question_encodings).T\n",
    "    \n",
    "    similarity_scores = np.dot(current_question_encoding, example_question_encodings)\n",
    "    # Sort indices by similarity scores in descending order\n",
    "    sorted_indices = np.argsort(-similarity_scores)\n",
    "\n",
    "    # Select the most similar examples\n",
    "    selected_examples = [few_shot_data[i] for i in sorted_indices[:num_examples]]\n",
    "\n",
    "    return selected_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first precompute all the vectors for the training set\n",
    "compute_question_encodings(splits['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this call will again take a few minutes, even if you batched; feel free to debug on smaller sets of dev\n",
    "predictions = predict_greedy_fewshot(gpt2_model, splits['test'], splits['train'], example_selection_method=select_similar_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-shot prompting with similar examples, execution acc: 0.34657039711191334\n"
     ]
    }
   ],
   "source": [
    "print('4-shot prompting with similar examples, execution acc:', check_execution_accuracy(predictions, splits['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now achieve about 34% accuracy. The autograder will check that you get >30%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_predictions(predictions, 'similar4shot_predictions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, inspect some of your predictions (from prompting with similar examples) compared to the correct outputs, and describe some common types of errors in your report. Are there any differences compared to the finetuned model, or are the types of errors pretty similar? You can also check the exact match accuracy again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension\n",
    "\n",
    "Finally, do some open-ended exploration to try to improve your performance on this dataset as much as possible (whether for finetuning or prompting). No hard requirement on how much to improve (or to improve at all), but please discuss the ideas you tried + how effective they were in your report. (Be careful with the GPU memory if you're using Kaggle, though- we're already nearly capping out the GPU memory in a few places with the current settings.)\n",
    "\n",
    "A non-exhaustive list of possible ideas:\n",
    "* Use a different similarity metric for selecting examples in few-shot prompting\n",
    "* Use more examples in few-shot prompting\n",
    "* Load a different base model than GPT2-Medium, scaling the model size, or look into calling the OpenAI API\n",
    "* Tune the hyperparameters used for finetuning\n",
    "* Try to combine few-shot prompting with finetuning\n",
    "\n",
    "**Show this part directly in your report.**\n",
    "\n",
    "Also, we would recommend you to do some reading on some prompting papers, since it is a hot topic in the domain. Here is some recommendation:\n",
    "\n",
    "* Language Models are Few-Shot Learners https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html\n",
    "* Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity https://arxiv.org/pdf/2104.08786.pdf\n",
    "* Finetuned Language Models Are Zero-Shot Learners https://arxiv.org/pdf/2109.01652.pdf\n",
    "* Making Pre-trained Language Models Better Few-shot Learners https://arxiv.org/pdf/2012.15723.pdf\n",
    "* Chain-of-Thought Prompting Elicits Reasoning in Large Language Models https://arxiv.org/pdf/2201.11903\n",
    "* RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning https://arxiv.org/abs/2205.12548\n",
    "* GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models https://arxiv.org/abs/2203.07281\n",
    "* Iterative Refinement with Self-Feedback https://arxiv.org/pdf/2303.17651.pdf\n",
    "\n",
    "\n",
    "If you have time and interests, we would recommend you to run their codes. Note that, some of the methods above do not work well on small language models like GPT-2 we used. We do not recommend you to run those costly experiments. If you want try it, you should consider small-size version LLMs like Pythia or GPTQ-version models. *If you can share your thoughts and experience on these in your presentation, we are sure it will be impressive.*\n",
    "\n",
    "**Show this part directly in your report.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your final submission should include the following files:\n",
    "\n",
    "* hw4.ipynb (this file; please rename to match)\n",
    "* finetuned_predictions.txt\n",
    "* similar4shot_predictions.txt\n",
    "* report.pdf Ôºàcan be markdown)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "py001",
   "language": "python",
   "name": "py001"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
